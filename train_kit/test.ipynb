{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random \n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import jieba\n",
    "from torchtext.data import TabularDataset,Field,BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return list(jieba.cut(text))\n",
    "\n",
    "TEXT = Field(sequential=True,tokenize=tokenize,lower = True,pad_token='<pad>',batch_first=True)\n",
    "fields = [('trg',TEXT),('src',TEXT)]\n",
    "\n",
    "train_data,valid_data = TabularDataset.splits(\n",
    "    path = 'C:\\\\Users\\\\Alfred\\\\Desktop\\\\rss\\\\train_kit\\\\data\\\\',\n",
    "    train = 'train.tsv',\n",
    "    validation = 'valiation.tsv',\n",
    "    format = 'tsv',\n",
    "    fields = fields,\n",
    ")\n",
    "TEXT.build_vocab(train_data,min_freq = 2)\n",
    "print(train_data)\n",
    "train_iter,valid_iter = BucketIterator.splits(\n",
    "    (train_data,valid_data),\n",
    "    batch_size = 8,\n",
    "    shuffle = True,\n",
    "    sort_key=lambda x: len(x.src),\n",
    "    device = -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2vocab = TEXT.vocab.itos\n",
    "vocab2id = TEXT.vocab.stoi\n",
    "PAD_IDX =vocab2id[TEXT.pad_token]\n",
    "SOS_IDX = vocab2id[TEXT.init_token]\n",
    "EOS_IDX = vocab2id[TEXT.eos_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,input_dim,emb_dim,enc_hid_dim,dec_hid_dim,dropout):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim,emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim,enc_hid_dim,bidirectional=True,batch_first =True)\n",
    "        self.fc = nn.Linear(enc_hid_dim *2,dec_hid_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        #embed->rnn->full_connected(dropout)\n",
    "        \n",
    "    def forward(self,src):\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        outputs,hidden = self.rnn(embedded)\n",
    "        hidden =torch.tanh(self.fc(torch.cat((hidden[-2,:,:],hidden[-1,:,:]),dim=1)))\n",
    "        return outputs,hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self,enc_hid_dim,dec_hid_dim):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Linear((enc_hid_dim*2)+dec_hid_dim,dec_hid_dim)\n",
    "        self.v = nn.Linear(dec_hid_dim,1,bias=False)\n",
    "    def forward(self,hidden,encoder_outputs):\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        hidden = hidden.unsqueeze(1).repeat(1,src_len,1)\n",
    "        energy = torch.tanh(self.attention(torch.cat((hidden,encoder_outputs),dim = 2)))\n",
    "        attention  = self.v(energy).squeeze(2)\n",
    "        return F.softmax(attention,dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, dropout, attention):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        self.attention = attention       \n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)        \n",
    "        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim, batch_first=True)        \n",
    "        self.fc_out = nn.Linear((enc_hid_dim * 2) + dec_hid_dim + emb_dim, output_dim)      \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, inputs, hidden, encoder_outputs):                    \n",
    "        inputs = inputs.unsqueeze(1)   \n",
    "        embedded = self.dropout(self.embedding(inputs))     \n",
    "        a = self.attention(hidden, encoder_outputs)                    \n",
    "        a = a.unsqueeze(1)        \n",
    "        weighted = torch.bmm(a, encoder_outputs)\n",
    "      \n",
    "        rnn_input = torch.cat((embedded, weighted), dim = 2)              \n",
    "        output, hidden = self.rnn(rnn_input, hidden.unsqueeze(0))             \n",
    "        embedded = embedded.squeeze(1)\n",
    "        output = output.squeeze(1)\n",
    "        weighted = weighted.squeeze(1)        \n",
    "        prediction = self.fc_out(torch.cat((output, weighted, embedded), dim = 1))             \n",
    "        return prediction, hidden.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self,encoder,decoder,device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim   \n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "        encoder_outputs, hidden = self.encoder(src)\n",
    "        inputs = trg[:,0]\n",
    "        \n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(inputs, hidden, encoder_outputs) \n",
    "            outputs[:,t,:] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            inputs = trg[:,t] if teacher_force else top1\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "INPUT_DIM = len(id2vocab)\n",
    "OUTPUT_DIM = len(id2vocab)\n",
    "ENC_EMB_DIM = 128\n",
    "DEC_EMB_DIM = 128\n",
    "ENC_HID_DIM = 256\n",
    "DEC_HID_DIM = 256\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPOUT = 0.5\n",
    "N_EPOCHS = 1\n",
    "CLIP = 1\n",
    "attention = Attention(ENC_HID_DIM,ENC_HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attention)\n",
    "model  =Seq2Seq(enc,dec,device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)    \n",
    "model.apply(init_weights)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(),lr = 5e-5)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index = PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.cuda.empty_cache()\n",
    "loss_vals = []\n",
    "loss_vals_eval = []\n",
    "\n",
    "device = torch.device('cpu')\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    model.train()\n",
    "    epoch_loss = []\n",
    "    dp = tqdm(valid_iter)\n",
    "    \n",
    "    for i ,batch in enumerate(dp):\n",
    "            \n",
    "        try:\n",
    "            src = batch.src.to(device)\n",
    "            trg = batch.trg.to(device)\n",
    "            model.zero_grad()\n",
    "            output = model(src, trg)     \n",
    "            output_dim = output.shape[-1]       \n",
    "            output = output[:, 1:, :].reshape(-1, output_dim)\n",
    "            trg = trg[:, 1:].reshape(-1)  \n",
    "            loss = criterion(output, trg)    \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "            epoch_loss.append(loss.item())\n",
    "            optimizer.step()\n",
    "        except StopIteration:\n",
    "            print(\"End of epoch\")\n",
    "            torch.cuda.empty_cache()\n",
    "            valid_iter.init_epoch()\n",
    "    loss_vals.append(np.mean(epoch_loss))\n",
    "    #节省eval步骤\n",
    "    print(f'Epoch: {epoch+1}, Loss: {np.mean(epoch_loss)}')\n",
    "torch.save(model.state_dict(), 'model.pt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
